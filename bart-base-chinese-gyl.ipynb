{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 9032244,
     "sourceType": "datasetVersion",
     "datasetId": 5444181
    },
    {
     "sourceId": 82197,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 69060,
     "modelId": 94187
    }
   ],
   "dockerImageVersionId": 30747,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import tensorflow as tf\nprint(tf.__version__)",
   "metadata": {
    "_uuid": "312e26d5-1f77-4edf-abd7-da6584431faf",
    "_cell_guid": "40eec243-68cd-4505-b684-57ec0e74ce76",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2024-07-26T06:30:13.990747Z",
     "iopub.execute_input": "2024-07-26T06:30:13.991384Z",
     "iopub.status.idle": "2024-07-26T06:30:13.998800Z",
     "shell.execute_reply.started": "2024-07-26T06:30:13.991350Z",
     "shell.execute_reply": "2024-07-26T06:30:13.997416Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "2.15.0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "! pip install datasets transformers rouge-score jieba",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T12:26:55.805322Z",
     "iopub.execute_input": "2024-07-25T12:26:55.806352Z",
     "iopub.status.idle": "2024-07-25T12:27:07.768673Z",
     "shell.execute_reply.started": "2024-07-25T12:26:55.806307Z",
     "shell.execute_reply": "2024-07-25T12:27:07.767704Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "! pip install --upgrade ipywidgets tqdm\n! jupyter nbextension enable --py widgetsnbextension",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T12:27:30.170634Z",
     "iopub.execute_input": "2024-07-25T12:27:30.171029Z",
     "iopub.status.idle": "2024-07-25T12:27:34.938879Z",
     "shell.execute_reply.started": "2024-07-25T12:27:30.170994Z",
     "shell.execute_reply": "2024-07-25T12:27:34.937640Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "! pip install lawrouge",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T13:01:39.187527Z",
     "iopub.execute_input": "2024-07-25T13:01:39.187882Z",
     "iopub.status.idle": "2024-07-25T13:01:54.600798Z",
     "shell.execute_reply.started": "2024-07-25T13:01:39.187855Z",
     "shell.execute_reply": "2024-07-25T13:01:54.599663Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "code",
   "source": "# 压缩下载训练完成的模型\nimport os\nimport zipfile\nimport datetime\n\ndef file2zip(packagePath, zipPath):\n    '''\n  :param packagePath: 文件夹路径\n  :param zipPath: 压缩包路径\n  :return:\n  '''\n    zip = zipfile.ZipFile(zipPath, 'w', zipfile.ZIP_DEFLATED)\n    for path, dirNames, fileNames in os.walk(packagePath):\n        fpath = path.replace(packagePath, '')\n        for name in fileNames:\n            fullName = os.path.join(path, name)\n            name = fpath + '\\\\' + name\n            zip.write(fullName, name)\n    zip.close()\n\n\nif __name__ == \"__main__\":\n    # 文件夹路径\n    packagePath = '/kaggle/working/results/best'#需要压缩的文件路径\n    zipPath = '/kaggle/working/output.zip'#保存压缩包的路径\n    if os.path.exists(zipPath):\n        os.remove(zipPath)\n    file2zip(packagePath, zipPath)\n    print(\"打包完成\")\n    print(datetime.datetime.utcnow())\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T17:54:07.894823Z",
     "iopub.execute_input": "2024-07-25T17:54:07.895567Z",
     "iopub.status.idle": "2024-07-25T17:54:43.382148Z",
     "shell.execute_reply.started": "2024-07-25T17:54:07.895533Z",
     "shell.execute_reply": "2024-07-25T17:54:43.381141Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": "打包完成\n2024-07-25 17:54:43.378698\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# 训练模型\nimport torch\nimport datasets\nimport lawrouge\nimport numpy as np\n\nfrom typing import List, Dict\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import (AutoTokenizer,\n                          AutoModelForSeq2SeqLM,\n                          DataCollatorForSeq2Seq,\n                          Seq2SeqTrainingArguments,\n                          Seq2SeqTrainer,\n                          BartForConditionalGeneration)\nbatch_size = 16\nepochs = 5\n# 最大输入长度\nmax_input_length = 512\n# 最大输出长度\nmax_target_length = 128\nlearning_rate = 1e-04\n# 读取数据\n\ndataset = load_dataset('json', data_files='/kaggle/input/nlpcc2017-data0/nlpcc2017_clean.json', field='data')\n# 加载tokenizer,中文bart使用bert的tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/bart-base-chinese/transformers/default/1/bart-base-chinese\")\n# 调整数据格式\ndef flatten(example):\n    return {\n        \"document\": example[\"content\"],\n        \"summary\": example[\"title\"],\n        \"id\": \"0\"\n    }\n\n# 将原始数据中的content和title转换为document和summary\ndataset = dataset[\"train\"].map(flatten, remove_columns=[\"title\", \"content\"])\nprint(dataset)\n# 划分数据集\ntrain_dataset, valid_dataset = dataset.train_test_split(test_size=0.1,shuffle=True,seed=42).values()\ntrain_dataset, test_dataset = train_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42).values()\ndatasets = datasets.DatasetDict({\"train\":train_dataset,\"validation\": valid_dataset,\"test\":test_dataset})\nprint(datasets[\"train\"][2])\nprint(datasets[\"validation\"][2])\nprint(datasets[\"test\"][2])\nprint(\"数据转换完毕\")\ndef preprocess_function(examples):\n    \"\"\"\n    document作为输入，summary作为标签\n    \"\"\"\n    inputs = [doc for doc in examples[\"document\"]]\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\ntokenized_datasets = datasets\ntokenized_datasets = tokenized_datasets.map(preprocess_function, batched=True, remove_columns=[\"document\", \"summary\", \"id\"])\n# print(tokenized_datasets[\"train\"][2].keys())\n# print(tokenized_datasets[\"train\"][2])\n\ndef collate_fn(features: Dict):\n    batch_input_ids = [torch.LongTensor(feature[\"input_ids\"]) for feature in features]\n    batch_attention_mask = [torch.LongTensor(feature[\"attention_mask\"]) for feature in features]\n    batch_labels = [torch.LongTensor(feature[\"labels\"]) for feature in features]\n\n    # padding\n    batch_input_ids = pad_sequence(batch_input_ids, batch_first=True, padding_value=0)\n    batch_attention_mask = pad_sequence(batch_attention_mask, batch_first=True, padding_value=0)\n    batch_labels = pad_sequence(batch_labels, batch_first=True, padding_value=-100)\n    return {\n        \"input_ids\": batch_input_ids,\n        \"attention_mask\": batch_attention_mask,\n        \"labels\": batch_labels\n    }\n\n# 构建DataLoader来验证collate_fn\ndataloader = DataLoader(tokenized_datasets[\"test\"], shuffle=False, batch_size=4, collate_fn=collate_fn)\nbatch = next(iter(dataloader))\n# print(batch)\n\nprint(\"开始模型训练\")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/input/bart-base-chinese/transformers/default/1/bart-base-chinese\")\n# output = model(**batch) # 验证前向传播\n# print(output)\nprint(\"加载预训练模型\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # 将id解码为文字\n    predictions = np.where(predictions != -100,predictions,tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # 替换标签中的-100\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    # 去掉解码后的空格\n    decoded_preds = [\"\".join(pred.replace(\" \", \"\")) for pred in decoded_preds]\n    decoded_labels = [\"\".join(label.replace(\" \", \"\")) for label in decoded_labels]\n    # 分词计算rouge\n    # decoded_preds = [\" \".join(jieba.cut(pred.replace(\" \", \"\"))) for pred in decoded_preds]\n    # decoded_labels = [\" \".join(jieba.cut(label.replace(\" \", \"\"))) for label in decoded_labels]\n    # 计算rouge\n    rouge = lawrouge.Rouge()\n    result = rouge.get_scores(decoded_preds, decoded_labels,avg=True)\n    result = {'rouge-1': result['rouge-1']['f'], 'rouge-2': result['rouge-2']['f'], 'rouge-l': result['rouge-l']['f']}\n    result = {key: value * 100 for key, value in result.items()}\n    return result\nearly_stopping=False\n# 设置训练参数\nargs = Seq2SeqTrainingArguments(\n    output_dir=\"results\", # 模型保存路径\n    num_train_epochs=epochs,\n    do_train=True,\n    do_eval=True,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    learning_rate=learning_rate,\n    warmup_steps=500,\n    weight_decay=0.001,\n    predict_with_generate=True,\n    logging_dir=\"logs\",\n    logging_steps=500,\n    eval_strategy=\"steps\",\n    save_total_limit=3,\n    generation_max_length=128, # 生成的最大长度\n    generation_num_beams=1, # beam search\n    load_best_model_at_end=True,\n    metric_for_best_model=\"rouge-1\"\n)\ntrainer = Seq2SeqTrainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=collate_fn,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)\ntrain_result = trainer.train()\n# 打印验证集上的结果\nprint(trainer.evaluate(tokenized_datasets[\"validation\"]))\n# 打印测试集上的结果\nprint(trainer.evaluate(tokenized_datasets[\"test\"]))\n# 保存最优模型\ntrainer.save_model(\"results/best\")\n\n\n# 加载训练好的模型\nmodel = BartForConditionalGeneration.from_pretrained(\"results/best\")\n# model = model.to(\"cuda\")\n# 从测试集中挑选4个样本\ntest_examples = test_dataset[\"document\"][:4]\nprint(test_examples)\ninputs = tokenizer(\n        test_examples,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_input_length,\n        return_tensors=\"pt\",\n    )\ninput_ids = inputs.input_ids.to(model.device)\nattention_mask = inputs.attention_mask.to(model.device)\n# 生成\noutputs = model.generate(input_ids, attention_mask=attention_mask, max_length=128)\n# 将token转换为文字\noutput_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\noutput_str = [s.replace(\" \",\"\") for s in output_str]\nprint(output_str)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T14:00:10.077842Z",
     "iopub.execute_input": "2024-07-25T14:00:10.078752Z",
     "iopub.status.idle": "2024-07-25T17:33:12.353545Z",
     "shell.execute_reply.started": "2024-07-25T14:00:10.078709Z",
     "shell.execute_reply": "2024-07-25T17:33:12.352415Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 加载训练好的模型  ----验证模型\nmodel = BartForConditionalGeneration.from_pretrained(\"results/best\")\n# model = model.to(\"cuda\")\n# 从测试集中挑选4个样本\ntest_examples = test_dataset[\"document\"][:4]\nprint(test_examples)\ninputs = tokenizer(\n        test_examples,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_input_length,\n        return_tensors=\"pt\",\n    )\ninput_ids = inputs.input_ids.to(model.device)\nattention_mask = inputs.attention_mask.to(model.device)\n# 生成\noutputs = model.generate(input_ids, attention_mask=attention_mask, max_length=128)\n# 将token转换为文字\noutput_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\noutput_str = [s.replace(\" \",\"\") for s in output_str]\nprint(output_str)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T18:01:56.884284Z",
     "iopub.execute_input": "2024-07-25T18:01:56.885158Z",
     "iopub.status.idle": "2024-07-25T18:02:06.344775Z",
     "shell.execute_reply.started": "2024-07-25T18:01:56.885117Z",
     "shell.execute_reply": "2024-07-25T18:02:06.343629Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "text": "['22大连市气象台2015年03月16日03时50分发布大雾橙色预警信号:预计16日凌晨到上午,大连市区、金州区及周边海域的局部地区将出现能见度小于200米的浓雾天气,请注意防范。图例标准防御指南6小时内可能出现能见度小于200米的浓雾,或者已经出现能见度小于200米、大于等于50米的浓雾且可能持续。1、有关部门和单位按照职责做好防雾工作;2、机场、高速公路、轮渡码头等单位加强调度指挥;3、驾驶人员必须严格控制车、船的行进速度;4、减少户外活动。', '12月24日,据成都全搜索报道,12月24日下午,成都市召开领导干部大会。省委常委、省委组织部部长范锐平宣布中央和四川省委关于唐良智同志任职的决定,唐良智任市委委员、常委、副书记,提名为成都市人民政府市长人选。范锐平代表省委书记王东明对成都工作提出四点要求。省委常委、市委书记黄新初主持会议并作重要讲话。唐良智1979年9月至1983年8月,华中工学院固体电子系半导体物理与器件专业学习;1983年8月至1985年4月,邮电部上海第一研究所工程技术人员;1985年4月至1987年1月,邮电部武汉邮电科学研究院工程技术人员;1987年1月至1991年2月,武汉市东湖新技术开发区管理办公室主任科员(1990年2月至1990年4月,赴日本JICA进修);1991年2月至1995年9月,东湖新技术开发区新技术创业中心副主任、党委书记;1995年9月至1998年7月,东湖新技术开发区管委会政策法规处副处长、处长(1992年9月至1995年12月,在华中理工大学西方经济学专业在职硕士研究生学习,获经济学硕士学位;1998年1月至1998年7月,兼任东湖高新技术股份公司总经理);1998年7月至2000年2月,东湖新技术开发区管委会副主任、党委委员;2000年2月至2001年2月,东湖新技术开发区管委会副主任、党委副书记(2000年10月明确正局级);2001年2月至2001年10月,市委武汉东湖新技术开发区工委副书记、管委会副主任;2001年10月至2003月4月,硚口区委副书记,区政府代理区长、区长(2002年1月任)(1999年9月至2002年6月,在华中科技大学西方经济学专业在职博士研究生学习,获经济学博士学位);2003年4月至2005年5月,武汉东湖新技术开发区管委会主任、工委副书记;2005年5月至2005年10月,武汉东湖新技术开发区管委会主任(副市级)、工委副书记;2005年10月至2007年6月,武汉东湖新技术开发区管委会主任(副市级)、工委书记;2007年6月至2008年2月,湖北省襄樊市委副书记,市政府代理市长、市长(2007年12月任);2008年2月至2011年3月,湖北省襄阳(樊)市委书记、市人大常委会主任(2008年4月任)(2008年9月至2009年1月,在中央党校中青班一班学习);2011年1月至2011年2月,武汉市委副书记,市政府代理市长;2011年2月至2014年12月,武汉市委副书记,市政府市长、党组书记;2014.12--成都市委副书记,市人民政府市长人选。第十一届全国人大代表。武汉市十一届市委委员,武汉市十二届人大代表。', '【环球时报综合报道】一则“中国女游客在菲律宾机场遭男性工作人员暴打”的视频近日在马尼拉华人圈引发愤怒。6日上午,中国驻菲律宾大使馆总领事邱舰等人赴菲律宾移民局向移民局局长米顺提出交涉,要求菲移民局彻查5日凌晨发生的这一事件。6日晚18时,《环球时报》记者通过电话联系到这名被打中国女子时,她称当时正在机场关押室,当晚晚些时候菲方面将决定将她遣返还是允许入关,但之后记者再拨打她的电话一直无法接通。据《环球时报》记者从菲律宾方面了解,她已被遣返回国。据这名江姓女子透露,她于5月4日上午7时左右搭乘班机抵达马尼拉尼蕊-阿基诺国际机场3号航站楼,在入关时受到菲移民局人员盘问,由于她的护照上有很多之前在菲律宾延期签证的盖章,移民局人员怀疑她虽然持旅游签证,但有在菲律宾非法务工之嫌,并要将她遣返回中国。江姓女子感到不服,认为自己此前虽然曾在菲律宾工作,但此次就是来旅游,仅凭护照上签证印章数目过多不足以构成立即遣返的条件,并与移民局工作人员理论,但移民局人员将她交给其他工作人员处理。后来,一位“穿着蓝色制服的人”强制将她关入机场内的海关待遣返区。江姓女子说,待遣返区中除她以外还有两名男子,其中一个是中国人、另一个是越南人,且卫生条件不佳,加上看守他们的是一名男士,导致她直到5日凌晨2时也不敢合眼。她说,此时自己已经身心俱疲,精神也有点恍惚,又感觉自己受到侮辱,于是拖着行李箱到关押室外找人理论,要求他们允许自己离开。这段由另一名乘客用手机拍摄的56秒视频显示,江姓女子被两男一女3名菲律宾工作人员在地面上拖行,其中一名男子穿着便衣但佩戴有身份证卡。江姓女子称,当时两个人拖着她,另外一个人在后面用脚踹她。她歇斯底里的叫喊“你为什么打我?”接着站起来并喊道“我跟你拼了”,冲向那名男性工作人员,并用其女式手包向他身上甩去,与后者扭打起来,该男子则连续给了江姓女子4个耳光,并把她强行推入关押室,两人继续在里面叫喊。其间,一名身穿制服的菲方女性工作人员推着红色的行李箱,一直目睹这一切,并未采取制止行为。这一视频被曝光后,引起很多当地华人的愤慨,纷纷在网上发帖要求中国驻菲使馆出面向菲政府交涉,要求放人和惩办打人的机场人员。中国驻菲律宾大使馆总领事邱舰6日对移民局长米顺表示,大使馆理解并尊重菲移民局依法做出的决定和执法行为,亦会引导教育本国公民遵法守法,但对于粗暴执法行为、损害中方人员合法权益的行为,中国驻菲律宾大使馆坚决反对,不能接受,菲移民局应对此事件做出负责任的处理。米顺表示,已经指示相关主管人员和部门彻查。任何国家来菲访客都应遵守菲律宾移民法律,菲律宾执法官员则必须公平公正执法,绝不允许借执法为名损害他人权益,一旦发生必须严肃处理,移民局将严肃认真处理此事。【环球时报驻菲律宾特约记者庄铭灯环球时报记者王盼盼】', '民警勘查现场。扬子晚报讯(通讯员徐旻记者郭靖宇)“我停在湖塘乐购超市顶楼停车场的车被砸了,东西翻得到处都是,你们快来看看啊!”14日清晨6点半,常州市武进区新城派出所的值班电话骤然响起,有市民报警称,车窗被砸,车内贵重物品也被偷走。据了解,当晚该停车场共有10辆私家车遭遇砸窗事件,警方已经锁定一名犯罪嫌疑人。市民蒋女士有一辆黑色的尼桑逍客轿车,事发前停在乐购超市顶楼的停车场里。14日早晨拿车时发现,汽车后挡风玻璃被砸,车上的物品被翻得乱七八糟,一片狼藉。经过清点,她发现,车上的几包中华香烟也不见了踪迹。接到报警后,常州市武进区新城派出所民警迅速赶到现场,经过走访发现,除了蒋女士的车之外,还有9辆车也遇到了类似情况,后挡风玻璃被砸破。据了解,当晚停在停车场过夜的车辆有40多辆,被砸的10辆车里有8辆是SUV,只有2辆是轿车,其中一辆是福克斯,另一辆是本田雅阁。民警分析,从现场情况来看,不法分子将目标锁定在后车窗砸破后能够翻进去的车辆,便于进入车内盗窃。目前警方已经锁定嫌疑人,不过因为可能涉及未成年人,因此并没有透露嫌疑人的更多细节。警方也提醒广大车主,车内在不放置贵重物品的前提下,也不要放置香烟、零钱、小包等物品,以免引起小偷注意。车辆停放请放置有人看守的地方。来源:扬子晚报']\n['大连市发布大雾橙色预警:预计16日凌晨到上午,市区、金州区及周边海域的局部地区将出现能见度小于200米的浓雾...', '唐良智任成都市委委员、常委、副书记,提名为市长人选。', '中国女游客在菲律宾机场遭男性工作人员暴打,当晚晚些时候菲方面将决定将她遣返还是允许入关。', '常州:超市顶楼停车场10辆私家车遭砸窗,车内贵重物品也被偷走,警方锁定嫌疑人']\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}
