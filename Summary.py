import time
import webbrowser
import mammoth
import streamlit as st
import datasets
from datasets import load_dataset
from transformers import (AutoTokenizer,
                          AutoModelForSeq2SeqLM,
                          DataCollatorForSeq2Seq,
                          Seq2SeqTrainingArguments,
                          Seq2SeqTrainer,
                          BartForConditionalGeneration)


def processing():
    word0 = st.empty()
    bar = st.progress(0)
    for i in range(100):
        word0.subheader('ç”Ÿæˆå¹¶è¾“å‡ºä¸­æ–‡æ‘˜è¦è¿›è¡Œä¸­......: ' + str(i + 1) + '%')
        bar.progress(i + 1)
        time.sleep(0.01)


def output_abstract():
    print("åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹")
    model = BartForConditionalGeneration.from_pretrained("./huggingface/best")
    # model = model.to("cuda")
    test_examples = test_dataset["document"][:10]
    # print(test_examples)
    st.subheader("ä¸­æ–‡æ‘˜è¦æµ‹è¯•é›†ï¼š")
    # st.text_area(label='', value=test_examples, height=300)
    st.write(test_examples)
    inputs = tokenizer(
            test_examples,
            padding="max_length",
            truncation=True,
            max_length=512,
            return_tensors="pt",
        )
    input_ids = inputs.input_ids.to(model.device)
    attention_mask = inputs.attention_mask.to(model.device)
    # ç”Ÿæˆ
    outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=128)
    # å°†tokenè½¬æ¢ä¸ºæ–‡å­—
    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    output_str = [s.replace(" ","") for s in output_str]
    # print(output_str)
    st.subheader("ç”Ÿæˆå¹¶è¾“å‡ºä¸­æ–‡æ‘˜è¦ï¼š")
    # st.text_area(label='', value=output_str, height=260)
    st.write(output_str)
    print("ä¸­æ–‡æ‘˜è¦è¾“å‡ºå®Œæˆ...")


def output_abstracts():
    tokenizer = AutoTokenizer.from_pretrained("./huggingface/bart-base-chinese")
    text = []
    st.write("å·²åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹./huggingface/best")
    print("åŠ è½½å®Œæ¯•è®­ç»ƒæ¨¡å‹")
    model = BartForConditionalGeneration.from_pretrained("./huggingface/best")
    st.subheader("é€‰æ‹©éœ€è¦ä¸­æ–‡æ–‡æœ¬æ‘˜è¦ç”Ÿæˆçš„æ–‡ä»¶ä¸Šä¼ ï¼š")
    uploader = st.file_uploader('', type=['txt', 'docx'])
    if uploader is not None:
        if uploader.name.split('.')[-1] == 'docx':
            text = mammoth.convert_to_markdown(uploader).value
        elif uploader.name.split('.')[-1] == 'txt':
            text = uploader.read().decode("utf-8")
    test_examples = text
    if test_examples:
        st.subheader("ä¸Šä¼ çš„ä¸­æ–‡æ–‡ç« ï¼š")
        st.text_area(label='', value=test_examples, height=300)
        inputs = tokenizer(
                test_examples,
                padding="max_length",
                truncation=True,
                max_length=512,
                return_tensors="pt",
            )
        input_ids = inputs.input_ids.to(model.device)
        attention_mask = inputs.attention_mask.to(model.device)
        # ç”Ÿæˆ
        outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=256)
        # å°†tokenè½¬æ¢ä¸ºæ–‡å­—
        output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        output_str = [s.replace(" ","") for s in output_str]
        print(output_str)
        print("ä¸­æ–‡æ‘˜è¦ç”Ÿæˆå®Œæ¯•")
        processing()
        st.text_area(label='', value=output_str[0], height=200)


def main():
    st.set_page_config(page_title="SummerTask",  page_icon="ğŸ¦ˆ", layout="wide", initial_sidebar_state="expanded",
                       menu_items={'Get Help': 'https://www.extremelycoolapp.com/help',
                                   'About': "# æš‘æœŸä»»åŠ¡"})
    st.sidebar.title('æš‘æœŸä»»åŠ¡')
    page = st.sidebar.selectbox("é€‰æ‹©ä¸€ä¸ªä»»åŠ¡", ["æ–‡æœ¬ä¸­æ–‡æ‘˜è¦ç”Ÿæˆ", "ä¸­æ–‡æ‘˜è¦ç”Ÿæˆè®­ç»ƒæŸ¥çœ‹", "è‹±æ–‡æ–‡æœ¬å•è¯é”™è¯¯æ ¡å¯¹", "ä¸­æ–‡äººåè¯†åˆ«-æ ‡æ³¨"])

    if page == "æ–‡æœ¬ä¸­æ–‡æ‘˜è¦ç”Ÿæˆ":
        st.title("æ–‡æœ¬ä¸­æ–‡æ‘˜è¦ç”Ÿæˆ")
        st.write("Tips:  åˆ©ç”¨Barté¢„è®­ç»ƒæ¨¡å‹å®Œæˆæ–‡æœ¬æ–‡ä»¶ï¼ˆdocxã€txtï¼‰ä¸€ä¸ªä¸­æ–‡æ‘˜è¦ç”Ÿæˆçš„ç¨‹åº")
        output_abstracts()
    elif page == "ä¸­æ–‡æ‘˜è¦ç”Ÿæˆè®­ç»ƒæŸ¥çœ‹":
        st.title("ä¸­æ–‡æ‘˜è¦ç”Ÿæˆè®­ç»ƒæŸ¥çœ‹")
        st.write("Tips:  åˆ©ç”¨Barté¢„è®­ç»ƒæ¨¡å‹ç¼–å†™ä¸€ä¸ªä¸­æ–‡æ‘˜è¦ç”Ÿæˆçš„ç¨‹åº")
        # st.text_area(label='è®­ç»ƒé›†ï¼š', value=datasets["train"][2], height=260)
        output_abstract()
    elif page == "è‹±æ–‡æ–‡æœ¬å•è¯é”™è¯¯æ ¡å¯¹":
        webbrowser.open("http://localhost:8501", new=0)
    elif page == "ä¸­æ–‡äººåè¯†åˆ«-æ ‡æ³¨":
        webbrowser.open("http://localhost:8501", new=0)

if __name__ == "__main__":
    dataset = load_dataset('json', data_files='./huggingface/nlpcc2017_clean.json', field='data')
    # åŠ è½½tokenizer,ä¸­æ–‡bartä½¿ç”¨bertçš„tokenizer
    tokenizer = AutoTokenizer.from_pretrained("./huggingface/bart-base-chinese")
    # è°ƒæ•´æ•°æ®æ ¼å¼
    def flatten(example):
        return {
            "document": example["content"],
            "summary": example["title"],
            "id": "0"
        }

    # å°†åŸå§‹æ•°æ®ä¸­çš„contentå’Œtitleè½¬æ¢ä¸ºdocumentå’Œsummary
    dataset = dataset["train"].map(flatten, remove_columns=["title", "content"])
    print(dataset)
    # åˆ’åˆ†æ•°æ®é›†
    train_dataset, valid_dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42).values()
    train_dataset, test_dataset = train_dataset.train_test_split(test_size=0.1, shuffle=True, seed=42).values()
    datasets = datasets.DatasetDict({"train": train_dataset, "validation": valid_dataset, "test": test_dataset})
    print("æ•°æ®è½¬æ¢å®Œæ¯•")
    main()